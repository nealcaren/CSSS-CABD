{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 3: Harvesting data from the web: APIs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A first API\n",
    "\n",
    "[Chronicling America](http://chroniclingamerica.loc.gov/about/) is a joint project of the National Endowment for the Humanities and the Library of Congress .\n",
    "\n",
    "Search for articles that mention \"[slavery](http://chroniclingamerica.loc.gov/search/pages/results/?andtext=slavery)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Look at the URL. What happens if you change the word slavery to abolition? \n",
    "\n",
    "What happens to the URL when you go to the second page? Can you get to page 251?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we append ``&format=json`` to the end of the search URL? \n",
    "\n",
    "\n",
    "http://chroniclingamerica.loc.gov/search/pages/results/?andtext=slavery&format=json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[``requests``](http://docs.python-requests.org/en/master/) is a useful and commonly used HTTP library for python. It is not a part of the default installation, but is included with Anaconda Python Distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It would be possible to use the API URL and parameters directly in the requests command, but since the most likely scenario involves making repeating calls to ``requests`` as part of a loop -- the search returned less than 1% of the results -- I store the strings first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base_url   = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = '?andtext=slavery&format=json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`requests.get()` is used for both accessing websites and APIs. The command can be modified by several arguements, but at a minimum, it requires the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r = requests.get(base_url + parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`r` is a `requests` response object. Any JSON returned by the server are stored in `.json().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "search_json = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "JSONs are dictionary like objects, in that they have keys (think variable names) and values. `.keys()` returns a list of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'totalItems', u'endIndex', u'startIndex', u'itemsPerPage', u'items']\n"
     ]
    }
   ],
   "source": [
    "print search_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can return the value of any key by putting the key name in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434349"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_json['totalItems']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "What else is in there? Where is the stuff we want?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As is often the case with results from an API, most of the keys and values are metadate about either the search or what is being returned. These are useful for knowing if the search is returning what you want, which is particularly important when you are making multiple calls to the API. \n",
    "\n",
    "The data I'm intereted in is all in `items`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print type(search_json['items'])\n",
    "print len(search_json['items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`items` is a list with 20 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(search_json['items'][0])\n",
    "print type(search_json['items'][19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each of the 20 items in the list is a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'sequence', u'county', u'edition', u'frequency', u'id', u'section_label', u'city', u'date', u'title', u'end_year', u'note', u'state', u'subject', u'type', u'place_of_publication', u'start_year', u'edition_label', u'publisher', u'language', u'alt_title', u'lccn', u'country', u'ocr_eng', u'batch', u'title_normal', u'url', u'place', u'page']\n"
     ]
    }
   ],
   "source": [
    "first_item = search_json['items'][0]\n",
    "\n",
    "print first_item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "What is the title of the first item?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "While a standard CSV file has a header row that describes the contents of each column, a JSON file has keys identifying the values found in each case. Importantly, these keys need not be the same for each item. Additionally, values don't have to be numbers of strings, but could be lists or dictionaries. For example, this JSON could have included a `newspaper` key that was a dictionary with all the metadata about the newspaper the article and issue was published, an `article` key that include the article specific information as another dictionary, and a `text` key whose value was a string with the article text.\n",
    "\n",
    "As before, we can examine the contents of a particular item, such as the publication's `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-slavery bugle. volume\n"
     ]
    }
   ],
   "source": [
    "print first_item['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The easiest way to view or analyze this data is to convert it to a dataset-like structure. While Python does not have a builting in dataframe type, the popular `pandas` library does. By convention, it is imported as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make sure all columns are displayed\n",
    "pd.set_option(\"display.max_columns\",101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "pandas is prety smart about importing different JSON-type objects and converting them to dataframes with its `.DataFrame()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt_title</th>\n",
       "      <th>batch</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_label</th>\n",
       "      <th>end_year</th>\n",
       "      <th>frequency</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>lccn</th>\n",
       "      <th>note</th>\n",
       "      <th>ocr_eng</th>\n",
       "      <th>page</th>\n",
       "      <th>place</th>\n",
       "      <th>place_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>section_label</th>\n",
       "      <th>sequence</th>\n",
       "      <th>start_year</th>\n",
       "      <th>state</th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>title_normal</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_ohi_ariel_ver02</td>\n",
       "      <td>[New Lisbon, Salem]</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>[Columbiana, Columbiana]</td>\n",
       "      <td>18490316</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1861</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/sn83035487/1849-03-16/ed-1/seq-1/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83035487</td>\n",
       "      <td>[Archived issues are available in digital form...</td>\n",
       "      <td>LAVE\\nam\\nJlile\\nVOL. 4. NO. 30.\\nSALEM. OHIO,...</td>\n",
       "      <td></td>\n",
       "      <td>[Ohio--Columbiana--New Lisbon, Ohio--Columbian...</td>\n",
       "      <td>New-Lisbon, Ohio</td>\n",
       "      <td>Ohio American Antislavery Society</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1845</td>\n",
       "      <td>[Ohio, Ohio]</td>\n",
       "      <td>[Antislavery movements--United States--Newspap...</td>\n",
       "      <td>Anti-slavery bugle. volume</td>\n",
       "      <td>anti-slavery bugle.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_golf_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19140516</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1914-05-16/ed-1/seq-10/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>r\\nmmmmmmmmmmmmmmmmmmmmmmmm\\n'SLAVERY RIFE IN ...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_india_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19161109</td>\n",
       "      <td>None</td>\n",
       "      <td>EXTRA</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1916-11-09/ed-1/seq-26/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>us remaining whites if we expect to\\nstay on t...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>26</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_golf_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19150327</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1915-03-27/ed-1/seq-24/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>THOUSANDS OF VEILED WOMEN OF TURKISH\\nHAREM ON...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>24</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_foxtrot_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130815</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-08-15/ed-1/seq-5/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>LOLA NORRiajQlVS SiENSAT-iPN AL t EVIDENCE IN ...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_foxtrot_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130308</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-03-08/ed-1/seq-6/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>that every possible weakness in. a\\ngirl as &amp;e...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  alt_title                     batch                 city   country  \\\n",
       "0        []     batch_ohi_ariel_ver02  [New Lisbon, Salem]      Ohio   \n",
       "1        []     batch_iune_golf_ver01            [Chicago]  Illinois   \n",
       "2        []    batch_iune_india_ver01            [Chicago]  Illinois   \n",
       "3        []     batch_iune_golf_ver01            [Chicago]  Illinois   \n",
       "4        []  batch_iune_foxtrot_ver01            [Chicago]  Illinois   \n",
       "5        []  batch_iune_foxtrot_ver01            [Chicago]  Illinois   \n",
       "\n",
       "                     county      date edition edition_label  end_year  \\\n",
       "0  [Columbiana, Columbiana]  18490316    None                    1861   \n",
       "1             [Cook County]  19140516    None  NOON EDITION      1917   \n",
       "2             [Cook County]  19161109    None         EXTRA      1917   \n",
       "3             [Cook County]  19150327    None  NOON EDITION      1917   \n",
       "4             [Cook County]  19130815    None                    1917   \n",
       "5             [Cook County]  19130308    None  NOON EDITION      1917   \n",
       "\n",
       "                            frequency  \\\n",
       "0                              Weekly   \n",
       "1  Daily (except Sunday and holidays)   \n",
       "2  Daily (except Sunday and holidays)   \n",
       "3  Daily (except Sunday and holidays)   \n",
       "4  Daily (except Sunday and holidays)   \n",
       "5  Daily (except Sunday and holidays)   \n",
       "\n",
       "                                         id   language        lccn  \\\n",
       "0   /lccn/sn83035487/1849-03-16/ed-1/seq-1/  [English]  sn83035487   \n",
       "1  /lccn/sn83045487/1914-05-16/ed-1/seq-10/  [English]  sn83045487   \n",
       "2  /lccn/sn83045487/1916-11-09/ed-1/seq-26/  [English]  sn83045487   \n",
       "3  /lccn/sn83045487/1915-03-27/ed-1/seq-24/  [English]  sn83045487   \n",
       "4   /lccn/sn83045487/1913-08-15/ed-1/seq-5/  [English]  sn83045487   \n",
       "5   /lccn/sn83045487/1913-03-08/ed-1/seq-6/  [English]  sn83045487   \n",
       "\n",
       "                                                note  \\\n",
       "0  [Archived issues are available in digital form...   \n",
       "1  [\"An adless daily newspaper.\", Archived issues...   \n",
       "2  [\"An adless daily newspaper.\", Archived issues...   \n",
       "3  [\"An adless daily newspaper.\", Archived issues...   \n",
       "4  [\"An adless daily newspaper.\", Archived issues...   \n",
       "5  [\"An adless daily newspaper.\", Archived issues...   \n",
       "\n",
       "                                             ocr_eng page  \\\n",
       "0  LAVE\\nam\\nJlile\\nVOL. 4. NO. 30.\\nSALEM. OHIO,...        \n",
       "1  r\\nmmmmmmmmmmmmmmmmmmmmmmmm\\n'SLAVERY RIFE IN ...        \n",
       "2  us remaining whites if we expect to\\nstay on t...        \n",
       "3  THOUSANDS OF VEILED WOMEN OF TURKISH\\nHAREM ON...        \n",
       "4  LOLA NORRiajQlVS SiENSAT-iPN AL t EVIDENCE IN ...        \n",
       "5  that every possible weakness in. a\\ngirl as &e...        \n",
       "\n",
       "                                               place place_of_publication  \\\n",
       "0  [Ohio--Columbiana--New Lisbon, Ohio--Columbian...     New-Lisbon, Ohio   \n",
       "1                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "2                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "3                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "4                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "5                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "\n",
       "                           publisher section_label  sequence  start_year  \\\n",
       "0  Ohio American Antislavery Society                       1        1845   \n",
       "1                       N.D. Cochran                      10        1911   \n",
       "2                       N.D. Cochran                      26        1911   \n",
       "3                       N.D. Cochran                      24        1911   \n",
       "4                       N.D. Cochran                       5        1911   \n",
       "5                       N.D. Cochran                       6        1911   \n",
       "\n",
       "          state                                            subject  \\\n",
       "0  [Ohio, Ohio]  [Antislavery movements--United States--Newspap...   \n",
       "1    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "2    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "3    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "4    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "5    [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "\n",
       "                        title         title_normal  type  \\\n",
       "0  Anti-slavery bugle. volume  anti-slavery bugle.  page   \n",
       "1               The day book.            day book.  page   \n",
       "2               The day book.            day book.  page   \n",
       "3               The day book.            day book.  page   \n",
       "4               The day book.            day book.  page   \n",
       "5               The day book.            day book.  page   \n",
       "\n",
       "                                                 url  \n",
       "0  http://chroniclingamerica.loc.gov/lccn/sn83035...  \n",
       "1  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "2  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "3  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "4  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "5  http://chroniclingamerica.loc.gov/lccn/sn83045...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(search_json['items'])\n",
    "\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that I converted `search_json['items']` to  dataframe and not the entire JSON file. This is because I wanted each row to be an article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endIndex</th>\n",
       "      <th>items</th>\n",
       "      <th>itemsPerPage</th>\n",
       "      <th>startIndex</th>\n",
       "      <th>totalItems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Columbiana', u'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 10, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 26, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 24, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 5, u'county': [u'Cook County'], ...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 6, u'county': [u'Cook County'], ...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 13, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [None], u'edition'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 30, u'county': [u'Cook County'],...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 4, u'county': [None], u'edition'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Washington'], u...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 2, u'county': [None], u'edition'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Ottawa'], u'edi...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Ashtabula'], u'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Rutland', u'Was...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 3, u'county': [u'Wood'], u'editi...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 4, u'county': [u'New York', u'Cu...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 10, u'county': [u'New York', u'C...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 1, u'county': [u'Washington'], u...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>{u'sequence': 2, u'county': [None], u'edition'...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>434349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    endIndex                                              items  itemsPerPage  \\\n",
       "0         20  {u'sequence': 1, u'county': [u'Columbiana', u'...            20   \n",
       "1         20  {u'sequence': 10, u'county': [u'Cook County'],...            20   \n",
       "2         20  {u'sequence': 26, u'county': [u'Cook County'],...            20   \n",
       "3         20  {u'sequence': 24, u'county': [u'Cook County'],...            20   \n",
       "4         20  {u'sequence': 5, u'county': [u'Cook County'], ...            20   \n",
       "5         20  {u'sequence': 6, u'county': [u'Cook County'], ...            20   \n",
       "6         20  {u'sequence': 13, u'county': [u'Cook County'],...            20   \n",
       "7         20  {u'sequence': 1, u'county': [None], u'edition'...            20   \n",
       "8         20  {u'sequence': 30, u'county': [u'Cook County'],...            20   \n",
       "9         20  {u'sequence': 4, u'county': [None], u'edition'...            20   \n",
       "10        20  {u'sequence': 1, u'county': [u'Washington'], u...            20   \n",
       "11        20  {u'sequence': 2, u'county': [None], u'edition'...            20   \n",
       "12        20  {u'sequence': 1, u'county': [u'Ottawa'], u'edi...            20   \n",
       "13        20  {u'sequence': 1, u'county': [u'Ashtabula'], u'...            20   \n",
       "14        20  {u'sequence': 1, u'county': [u'Rutland', u'Was...            20   \n",
       "15        20  {u'sequence': 3, u'county': [u'Wood'], u'editi...            20   \n",
       "16        20  {u'sequence': 4, u'county': [u'New York', u'Cu...            20   \n",
       "17        20  {u'sequence': 10, u'county': [u'New York', u'C...            20   \n",
       "18        20  {u'sequence': 1, u'county': [u'Washington'], u...            20   \n",
       "19        20  {u'sequence': 2, u'county': [None], u'edition'...            20   \n",
       "\n",
       "    startIndex  totalItems  \n",
       "0            1      434349  \n",
       "1            1      434349  \n",
       "2            1      434349  \n",
       "3            1      434349  \n",
       "4            1      434349  \n",
       "5            1      434349  \n",
       "6            1      434349  \n",
       "7            1      434349  \n",
       "8            1      434349  \n",
       "9            1      434349  \n",
       "10           1      434349  \n",
       "11           1      434349  \n",
       "12           1      434349  \n",
       "13           1      434349  \n",
       "14           1      434349  \n",
       "15           1      434349  \n",
       "16           1      434349  \n",
       "17           1      434349  \n",
       "18           1      434349  \n",
       "19           1      434349  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(search_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If this dataframe contained all the items that you were looking for, it would be easy to save this to a csv file for storage and later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('lynching_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",alt_title,batch,city,country,county,date,edition,edition_label,end_year,frequency,id,language,lccn,note,ocr_eng,page,place,place_of_publication,publisher,section_label,sequence,start_year,state,subject,title,title_normal,type,url\r\n",
      "0,[],batch_ohi_ariel_ver02,\"[u'New Lisbon', u'Salem']\",Ohio,\"[u'Columbiana', u'Columbiana']\",18490316,,,1861,Weekly,/lccn/sn83035487/1849-03-16/ed-1/seq-1/,[u'English'],sn83035487,\"[u'Archived issues are available in digital format as part of the Library of Congress Chronicling America online collection.', u'Editors: Benjamin S. Jones, J. Elizabeth Hitchcock, 1845-1846; Benjamin S. Jones, J. Elizabeth Jones, 1846-1849; Oliver Johnson 1849-1851; Marius R. Robinson, 1851-1859; Benjamin S. Jones, 1859-1861.', u'Not published June 27-July 18, 1845.', u'Printers: John Frost, 1845; J.H. Painter, 1845-1846; G.N. Hapgood, 1846-1848.', u'Published in: New Lisbon, Ohio, June 20, 1845-Aug. 29, 1845, and: Salem, Ohio, Sept. 5, 1845-May 4, 1861.', u'Publisher: Executive Committee of the Western Anti-slavery Society, 1848-1861.']\",\"LAVE\r\n",
      "am\r\n",
      "Jlile\r\n",
      "VOL. 4. NO. 30.\r\n",
      "SALEM. OHIO, FRIDAY, MARCH 1G, 1849.\r\n",
      "WHOLE NO. 186.\r\n",
      "ANTI\r\n",
      "Ti v Tr-\r\n",
      "HI\r\n"
     ]
    }
   ],
   "source": [
    "!head lynching_articles.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is only a small subset of the articles on lynching that are available, however. The API returns results in batches of 20 and this is only the first page of results. As is often the case, I'll need to make multiple calls to the API to retrieve all the data of interest. The easiest way to do that is to define a small function for getting the article information and put that in a loop. While it isn't a requirement that you create a function for making the API call, it will make your code easier to read and debug.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Looking at the API guidelines, there is an additional paramater `page` that tells the API which subset of results we want. This name varies by API but their is usually some mechanism for retrieiving results beyond the initial JSON.\n",
    "\n",
    "Before creating the loop and making multiple calls to the API, I want to make sure that the API is working the way I think it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Look at the API guidelines. How can we get the third page?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "base_url   = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = '?andtext=slavery&format=json&page=3'\n",
    "\n",
    "r = requests.get(base_url + parameters)\n",
    "results =  r.json()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A call to random selected page 3 returns results 41 through 60, which is what I expected since each page has 20 items.\n",
    "\n",
    "The parameters are getting pretty ugly, so fortunately `requests` accepts a dictionary where the keys are the parameter names as defined by the API and the values are the search paramaters you are looking for. So the same request can be rewritten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = {'andtext': 'lynching',\n",
    "              'page' : 3,\n",
    "              'format'  : 'json'}\n",
    "r = requests.get(base_url, params=parameters)\n",
    "\n",
    "results =  r.json()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This can be rewritten as function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_articles():\n",
    "    '''\n",
    "    Make calls to the Chronicling America API.\n",
    "    '''\n",
    "    \n",
    "    base_url   = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "    parameters = {'andtext': 'lynching',\n",
    "                  'page'   : 3,\n",
    "                  'format' : 'json'}\n",
    "    \n",
    "    r = requests.get(base_url, params = parameters)\n",
    "    results =  r.json()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "results = get_articles()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The advantage of writing a function, however, would be that you can pass along your own parameters, such as the search term and page number, which would make this much more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_articles(search_term, page_number):\n",
    "    '''\n",
    "    Make calls to the Chronicling America API.\n",
    "    '''\n",
    "    \n",
    "    base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "    parameters = {'andtext': search_term,\n",
    "                  'page'   : page_number,\n",
    "                  'format' : 'json'}\n",
    "    \n",
    "    r = requests.get(base_url, params = parameters)\n",
    "    results =  r.json()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "results = get_articles('lynching', 3)\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, the first 60 results could downloaded in a just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 20\n",
      "21 40\n",
      "41 60\n"
     ]
    }
   ],
   "source": [
    "for page_number in range(1,4): # range stops before it gets to the last number\n",
    "    results = get_articles('lynching', page_number)\n",
    "    print results['startIndex'], results['endIndex']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Everything appears to be working, but unfortunately I only have the last page of results still. Each call to the API was redefining `results` variable. In this case, I set up an empty dataframe to store the results and will append the items from each page of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt_title</th>\n",
       "      <th>batch</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_label</th>\n",
       "      <th>end_year</th>\n",
       "      <th>frequency</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>lccn</th>\n",
       "      <th>note</th>\n",
       "      <th>ocr_eng</th>\n",
       "      <th>page</th>\n",
       "      <th>place</th>\n",
       "      <th>place_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>section_label</th>\n",
       "      <th>sequence</th>\n",
       "      <th>start_year</th>\n",
       "      <th>state</th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>title_normal</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_mimtptc_jackson_ver01</td>\n",
       "      <td>[Dearborn]</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[Wayne]</td>\n",
       "      <td>19211022</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1927</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/2013218776/1921-10-22/ed-1/seq-1/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>2013218776</td>\n",
       "      <td>[\"The Ford international weekly\" appears with ...</td>\n",
       "      <td>\"Mis-Picturing Us Abroad\" Introduces the Serie...</td>\n",
       "      <td></td>\n",
       "      <td>[Michigan--Wayne--Dearborn]</td>\n",
       "      <td>Dearborn, Mich.</td>\n",
       "      <td>Suburban Pub. Co.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "      <td>[Michigan]</td>\n",
       "      <td>[Dearborn (Mich.)--Newspapers., Michigan--Dear...</td>\n",
       "      <td>Dearborn independent.</td>\n",
       "      <td>dearborn independent.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/2013218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_hotel_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19150818</td>\n",
       "      <td>None</td>\n",
       "      <td>LAST EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1915-08-18/ed-1/seq-4/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>25 patriots who took into their own\\nhands a l...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Saint Paul tidende]</td>\n",
       "      <td>batch_mnhi_gemma_ver01</td>\n",
       "      <td>[Saint Paul]</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>[Ramsey]</td>\n",
       "      <td>19160310</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1928</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/sn90059649/1916-03-10/ed-1/seq-4/</td>\n",
       "      <td>[Danish]</td>\n",
       "      <td>sn90059649</td>\n",
       "      <td>[Available on microfilm from the Minnesota His...</td>\n",
       "      <td>}'i Room 201 Court Six#, St. Paul, fflinn.\\nB...</td>\n",
       "      <td></td>\n",
       "      <td>[Minnesota--Ramsey--Saint Paul]</td>\n",
       "      <td>St. Paul, Minn.</td>\n",
       "      <td>C. Rasmussen Pub. Co.</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>1902</td>\n",
       "      <td>[Minnesota]</td>\n",
       "      <td>[Danes--Minnesota--Newspapers., Danes.--fast--...</td>\n",
       "      <td>St. Paul tidende.</td>\n",
       "      <td>st. paul tidende.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn90059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Saint Paul tidende]</td>\n",
       "      <td>batch_mnhi_gemma_ver01</td>\n",
       "      <td>[Saint Paul]</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>[Ramsey]</td>\n",
       "      <td>19091119</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1928</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/sn90059649/1909-11-19/ed-1/seq-5/</td>\n",
       "      <td>[Danish]</td>\n",
       "      <td>sn90059649</td>\n",
       "      <td>[Available on microfilm from the Minnesota His...</td>\n",
       "      <td>November 1909.\\nLynching i Illinois.\\nMinnesot...</td>\n",
       "      <td></td>\n",
       "      <td>[Minnesota--Ramsey--Saint Paul]</td>\n",
       "      <td>St. Paul, Minn.</td>\n",
       "      <td>C. Rasmussen Pub. Co.</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>1902</td>\n",
       "      <td>[Minnesota]</td>\n",
       "      <td>[Danes--Minnesota--Newspapers., Danes.--fast--...</td>\n",
       "      <td>St. Paul tidende.</td>\n",
       "      <td>st. paul tidende.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn90059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Star, Sunday star]</td>\n",
       "      <td>batch_dlc_dalek_ver01</td>\n",
       "      <td>[Washington]</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>[None]</td>\n",
       "      <td>19221123</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1972</td>\n",
       "      <td>Daily</td>\n",
       "      <td>/lccn/sn83045462/1922-11-23/ed-1/seq-34/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045462</td>\n",
       "      <td>[\"From April 25 through May 24, 1861 one sheet...</td>\n",
       "      <td>T\\nTU^\\nnit;\\n- V E\\nxl\\nII\\nb&lt;\\nin rour\\n3436...</td>\n",
       "      <td>34</td>\n",
       "      <td>[District of Columbia--Washington]</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>W.D. Wallach &amp; Hope</td>\n",
       "      <td></td>\n",
       "      <td>34</td>\n",
       "      <td>1854</td>\n",
       "      <td>[District of Columbia]</td>\n",
       "      <td>[Washington (D.C.)--fast--(OCoLC)fst01204505, ...</td>\n",
       "      <td>Evening star.</td>\n",
       "      <td>evening star.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              alt_title                        batch          city  \\\n",
       "0                    []  batch_mimtptc_jackson_ver01    [Dearborn]   \n",
       "1                    []       batch_iune_hotel_ver01     [Chicago]   \n",
       "2  [Saint Paul tidende]       batch_mnhi_gemma_ver01  [Saint Paul]   \n",
       "3  [Saint Paul tidende]       batch_mnhi_gemma_ver01  [Saint Paul]   \n",
       "4   [Star, Sunday star]        batch_dlc_dalek_ver01  [Washington]   \n",
       "\n",
       "                country         county      date edition edition_label  \\\n",
       "0              Michigan        [Wayne]  19211022    None                 \n",
       "1              Illinois  [Cook County]  19150818    None  LAST EDITION   \n",
       "2             Minnesota       [Ramsey]  19160310    None                 \n",
       "3             Minnesota       [Ramsey]  19091119    None                 \n",
       "4  District of Columbia         [None]  19221123    None                 \n",
       "\n",
       "   end_year                           frequency  \\\n",
       "0      1927                              Weekly   \n",
       "1      1917  Daily (except Sunday and holidays)   \n",
       "2      1928                              Weekly   \n",
       "3      1928                              Weekly   \n",
       "4      1972                               Daily   \n",
       "\n",
       "                                         id   language        lccn  \\\n",
       "0   /lccn/2013218776/1921-10-22/ed-1/seq-1/  [English]  2013218776   \n",
       "1   /lccn/sn83045487/1915-08-18/ed-1/seq-4/  [English]  sn83045487   \n",
       "2   /lccn/sn90059649/1916-03-10/ed-1/seq-4/   [Danish]  sn90059649   \n",
       "3   /lccn/sn90059649/1909-11-19/ed-1/seq-5/   [Danish]  sn90059649   \n",
       "4  /lccn/sn83045462/1922-11-23/ed-1/seq-34/  [English]  sn83045462   \n",
       "\n",
       "                                                note  \\\n",
       "0  [\"The Ford international weekly\" appears with ...   \n",
       "1  [\"An adless daily newspaper.\", Archived issues...   \n",
       "2  [Available on microfilm from the Minnesota His...   \n",
       "3  [Available on microfilm from the Minnesota His...   \n",
       "4  [\"From April 25 through May 24, 1861 one sheet...   \n",
       "\n",
       "                                             ocr_eng page  \\\n",
       "0  \"Mis-Picturing Us Abroad\" Introduces the Serie...        \n",
       "1  25 patriots who took into their own\\nhands a l...        \n",
       "2  }'i Room 201 Court Six#, St. Paul, fflinn.\\nB...        \n",
       "3  November 1909.\\nLynching i Illinois.\\nMinnesot...        \n",
       "4  T\\nTU^\\nnit;\\n- V E\\nxl\\nII\\nb<\\nin rour\\n3436...   34   \n",
       "\n",
       "                                place place_of_publication  \\\n",
       "0         [Michigan--Wayne--Dearborn]      Dearborn, Mich.   \n",
       "1    [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "2     [Minnesota--Ramsey--Saint Paul]      St. Paul, Minn.   \n",
       "3     [Minnesota--Ramsey--Saint Paul]      St. Paul, Minn.   \n",
       "4  [District of Columbia--Washington]     Washington, D.C.   \n",
       "\n",
       "               publisher section_label  sequence  start_year  \\\n",
       "0      Suburban Pub. Co.                       1        1901   \n",
       "1           N.D. Cochran                       4        1911   \n",
       "2  C. Rasmussen Pub. Co.                       4        1902   \n",
       "3  C. Rasmussen Pub. Co.                       5        1902   \n",
       "4    W.D. Wallach & Hope                      34        1854   \n",
       "\n",
       "                    state                                            subject  \\\n",
       "0              [Michigan]  [Dearborn (Mich.)--Newspapers., Michigan--Dear...   \n",
       "1              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "2             [Minnesota]  [Danes--Minnesota--Newspapers., Danes.--fast--...   \n",
       "3             [Minnesota]  [Danes--Minnesota--Newspapers., Danes.--fast--...   \n",
       "4  [District of Columbia]  [Washington (D.C.)--fast--(OCoLC)fst01204505, ...   \n",
       "\n",
       "                   title           title_normal  type  \\\n",
       "0  Dearborn independent.  dearborn independent.  page   \n",
       "1          The day book.              day book.  page   \n",
       "2      St. Paul tidende.      st. paul tidende.  page   \n",
       "3      St. Paul tidende.      st. paul tidende.  page   \n",
       "4          Evening star.          evening star.  page   \n",
       "\n",
       "                                                 url  \n",
       "0  http://chroniclingamerica.loc.gov/lccn/2013218...  \n",
       "1  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "2  http://chroniclingamerica.loc.gov/lccn/sn90059...  \n",
       "3  http://chroniclingamerica.loc.gov/lccn/sn90059...  \n",
       "4  http://chroniclingamerica.loc.gov/lccn/sn83045...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame() # empty dataframe to store results\n",
    "\n",
    "for page_number in range(1,4):\n",
    "    results = get_articles('lynching', page_number)\n",
    "    new_df = pd.DataFrame(results['items'])\n",
    "    df = df.append(new_df , ignore_index=True) # otherwise, index would be 0-20 3x\n",
    "    \n",
    "print len(df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a large download, you would still want to tweak this a bit by pausing between each API call and making it robust to internet or API errors, but this is a solid framework for collecting data from an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h1> Geocoding</h1>\n",
    "\n",
    "Work in groups of three!\n",
    "<p>\n",
    "You have been handed a list of addresses. You want to geocode them. \n",
    "<p>Read about the Google Maps Geocoding API.\n",
    "<p>On paper, map out your work flow. What functions will you need?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "File location:\n",
    "\n",
    "https://raw.githubusercontent.com/nealcaren/CSSS-CABD/master/files/locations.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "My workflow:   \n",
    "   \n",
    "   \n",
    "1. Use `requests` to test out a single address.    \n",
    "2. Turn that into a function that accepts a location.    \n",
    "3. Read in the CSV file with all the locations.    \n",
    "4. Store the results    \n",
    "5. Write them to a CSV.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A third API\n",
    "\n",
    "While the Chronicling America API allows annonymous usage, most APIs require you to register in advance. This usually involves going to their website, signing up for the service, and then going through a second signup for developers.  \n",
    "\n",
    "\n",
    "When you sign up  to use an API, you usually agree to only use the API to facilitate other people using the service (e.g. customer's finding their way to your store) and that you won't store the data. API providers usually enforce this through rate limiting, meaning you can only access the service so many times per minute or per day. For example, you can only search status updates 180 times every 15 minutes according to [Twitter guidelines](https://dev.twitter.com/docs/rate-limiting/1.1/limits). [Yelp](http://www.yelp.com/developers/documentation/faq) limits you to 10,000 calls per day. If you go over your limit, you won't be able to access the service for a bit. You will also get in trouble if you redistribute the data, so don't plan on doing that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Two of the major reasons that web services require API authentication is so that they know who you are and so they can make sure that you don't go over their rate limits. Since you shouldn't be giving your password to random people on the internet, API authentication works a little bit differently. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like many other places, in order to use the Yelp API you have to sign up as [developer](http://www.yelp.com/developers). After telling them a little bit about what you plan to do--feel free to be honest; they aren't going to deny you access if you put \"research on food cultures\" as the purpose--you will get a *Consumer Key*, *Consumer Secret*, *Token*, and *Token Secret*. Copy and paste them somewhere special. \n",
    "\n",
    "Using the Yelp API goes something like this. First, you tell Yelp who you are and what you want. Assuming you are authorized to have this information, they respond with a URL where you can retrieve the data. The coding for this in practice is a little bit complicated, so there are often single use tools for accessing APIs, like [Tweepy](http://tweepy.github.io) for Twitter. \n",
    "\n",
    "Yelp uses the OAuth protocol for authentication. There are several python libraries for handling this, but you will likely need to install one (via `conda` or `pip`) yourself first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import oauth2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "There's no module to install for the Yelp API, but Yelp does provide some [sample Python code](https://github.com/Yelp/yelp-api/tree/master/v2/python). I've slightly modified the code below to show a sample search for restaurants near Chapel Hill, NC, sorted by distance. You can find more options in the search [documentation](http://www.yelp.com/developers/documentation/v2/search_api). The API's search options include things like location and type of business, and allows you to sort either by distance or popularity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "consumer_key    = 'qDBPo9c_szHVrZwxzo-zDw'\n",
    "consumer_secret = '4we8Jz9rq5J3j15Z5yCUqmgDJjM'\n",
    "token           = 'jeRrhRey_k-emvC_VFLGrlVHrkR4P3UF'\n",
    "token_secret    = 'n-7xHNCxxedmAMYZPQtnh1hd7lI'\n",
    "\n",
    "consumer = oauth2.Consumer(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://api.yelp.com/v2/search?sort=1&oauth_body_hash=2jmj7l5rSw0yVb%2FvlWAYkK%2FYBwk%3D&oauth_nonce=13853216&oauth_timestamp=1501417028&oauth_consumer_key=qDBPo9c_szHVrZwxzo-zDw&oauth_signature_method=HMAC-SHA1&category_filter=restaurants&oauth_token=jeRrhRey_k-emvC_VFLGrlVHrkR4P3UF&location=Oslo%2C+Norway&oauth_signature=we0CxC12Un504SHisw3Pk0xJBp8%3D\n"
     ]
    }
   ],
   "source": [
    "category_filter = 'restaurants'\n",
    "location        = 'Oslo, Norway'\n",
    "\n",
    "options         =  'category_filter=%s&location=%s&sort=1' % (category_filter, location)\n",
    "url             = 'http://api.yelp.com/v2/search?' + options\n",
    "\n",
    "oauth_request = oauth2.Request('GET', url, {})\n",
    "\n",
    "oauth_request.update({'oauth_nonce'      : oauth2.generate_nonce(),\n",
    "                      'oauth_timestamp'  : oauth2.generate_timestamp(),\n",
    "                      'oauth_token'       : token,\n",
    "                      'oauth_consumer_key': consumer_key})\n",
    "\n",
    "token = oauth2.Token(token, token_secret)\n",
    "oauth_request.sign_request(oauth2.SignatureMethod_HMAC_SHA1(), consumer, token)\n",
    "signed_url = oauth_request.to_url()\n",
    "\n",
    "print signed_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The URL returned expires after a couple of seconds, so don't expect for the above link to work. The results are provided in the JSON file format, so I'm going to use the already imported `requests` module to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'region', u'total', u'businesses']\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(url=signed_url)\n",
    "oslo_restaurants = resp.json()\n",
    "\n",
    "print oslo_restaurants.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As with the Chronacling America API, the top level of the JSON contains some metadata about the search with all the specific items in one field. In this case, `businesses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'categories': [[u'Scandinavian', u'scandinavian']],\n",
       " u'display_phone': u'+47 22 69 60 00',\n",
       " u'id': u'smalhans-oslo',\n",
       " u'image_url': u'https://s3-media1.fl.yelpcdn.com/bphoto/5QT795Zc4TcUl8ue-iA-Og/ms.jpg',\n",
       " u'is_claimed': True,\n",
       " u'is_closed': False,\n",
       " u'location': {u'address': [u'Waldemar Thranes gate 10'],\n",
       "  u'city': u'Oslo',\n",
       "  u'coordinate': {u'latitude': 59.9235229, u'longitude': 10.7395983},\n",
       "  u'country_code': u'NO',\n",
       "  u'display_address': [u'Waldemar Thranes gate 10',\n",
       "   u'St. Hanshaugen',\n",
       "   u'0171 Oslo',\n",
       "   u'Norway'],\n",
       "  u'geo_accuracy': 8.0,\n",
       "  u'neighborhoods': [u'St. Hanshaugen', u'Bislett'],\n",
       "  u'postal_code': u'0171',\n",
       "  u'state_code': u'03'},\n",
       " u'mobile_url': u'https://m.yelp.com/biz/smalhans-oslo?adjust_creative=qDBPo9c_szHVrZwxzo-zDw&utm_campaign=yelp_api&utm_medium=api_v2_search&utm_source=qDBPo9c_szHVrZwxzo-zDw',\n",
       " u'name': u'Smalhans',\n",
       " u'phone': u'+4722696000',\n",
       " u'rating': 4.0,\n",
       " u'rating_img_url': u'https://s3-media4.fl.yelpcdn.com/assets/2/www/img/c2f3dd9799a5/ico/stars/v1/stars_4.png',\n",
       " u'rating_img_url_large': u'https://s3-media2.fl.yelpcdn.com/assets/2/www/img/ccf2b76faa2c/ico/stars/v1/stars_large_4.png',\n",
       " u'rating_img_url_small': u'https://s3-media4.fl.yelpcdn.com/assets/2/www/img/f62a5be2f902/ico/stars/v1/stars_small_4.png',\n",
       " u'review_count': 39,\n",
       " u'snippet_image_url': u'https://s3-media2.fl.yelpcdn.com/photo/CxQBlSGNnDqFmCu9rOUgnA/ms.jpg',\n",
       " u'snippet_text': u\"My wife and I made this our first culinary stop on our 7 day Norway trip and it didn't disappoint. The reason I chose it was it was the least stodgy...\",\n",
       " u'url': u'https://www.yelp.com/biz/smalhans-oslo?adjust_creative=qDBPo9c_szHVrZwxzo-zDw&utm_campaign=yelp_api&utm_medium=api_v2_search&utm_source=qDBPo9c_szHVrZwxzo-zDw'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oslo_restaurants['businesses'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Inspecting the returned results for one restaraunt, it is clear that Yelp is keeping a lot of the review data for themselves. They returned the overall restaurant `rating`, but they provide only a small bit of text (`snippet_text`) instead of the full reviews and ratings. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print oslo_restaurants['total']\n",
    "print len(oslo_restaurants['businesses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Additionally, they cap the total number of business the search will return at 40 and only provide 20 results for each API call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with these restrictions, it still might be useful for social science research. As before, you would likely want to define a function in order to make repeated calls to the API. In this, the easier solution might be to create two functions. One that gets a single page and another which retrieves both pages for a single geographical area by calling the first function twice. While it would be possible to do this with zero or one new functions, creating two functions allows for better control over finding and debugging errors since you can test each function independently. Creating lots of small functions generally the code more readable, especially in case like this where you are looping over pages within restaurants within geographic areas. In general, I think the principle of a workflow consisting of small functions, as is commonly found in Python code, is something that social scientists should adopt even when they aren't writing Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_yelp_page(location, offset):\n",
    "    '''\n",
    "    Retrieve one page of results from the Yelp API\n",
    "    Returns a JSON file\n",
    "    '''\n",
    "    # from https://github.com/Yelp/yelp-api/tree/master/v2/python\n",
    "    consumer_key    = 'qDBPo9c_szHVrZwxzo-zDw'\n",
    "    consumer_secret = '4we8Jz9rq5J3j15Z5yCUqmgDJjM'\n",
    "    token           = 'jeRrhRey_k-emvC_VFLGrlVHrkR4P3UF'\n",
    "    token_secret    = 'n-7xHNCxxedmAMYZPQtnh1hd7lI'\n",
    "    \n",
    "    consumer = oauth2.Consumer(consumer_key, consumer_secret)\n",
    "    \n",
    "    url = 'http://api.yelp.com/v2/search?category_filter=restaurants&location=%s&sort=1&offset=%s' % (location, offset)\n",
    "    \n",
    "    oauth_request = oauth2.Request('GET', url, {})\n",
    "    oauth_request.update({'oauth_nonce': oauth2.generate_nonce(),\n",
    "                          'oauth_timestamp': oauth2.generate_timestamp(),\n",
    "                          'oauth_token': token,\n",
    "                          'oauth_consumer_key': consumer_key})\n",
    "    \n",
    "    token = oauth2.Token(token, token_secret)\n",
    "    \n",
    "    oauth_request.sign_request(oauth2.SignatureMethod_HMAC_SHA1(), consumer, token)\n",
    "    \n",
    "    signed_url = oauth_request.to_url()\n",
    "    resp = requests.get(url=signed_url)\n",
    "    return resp.json()\n",
    "\n",
    "def get_yelp_results(location):\n",
    "    '''\n",
    "    Retrive both pages of results from the Yelp API\n",
    "    Returns a dataframe\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    for offset in [1,21]:\n",
    "        results = get_yelp_page(location, offset)\n",
    "        new_df = pd.DataFrame(results['businesses'])\n",
    "        df = df.append(new_df , ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "ch_df = get_yelp_results('Chapel Hill, NC')\n",
    "\n",
    "print len(ch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'categories', u'display_phone', u'id', u'image_url', u'is_claimed',\n",
       "       u'is_closed', u'location', u'menu_date_updated', u'menu_provider',\n",
       "       u'mobile_url', u'name', u'phone', u'rating', u'rating_img_url',\n",
       "       u'rating_img_url_large', u'rating_img_url_small', u'review_count',\n",
       "       u'snippet_image_url', u'snippet_text', u'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>review_count</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>George's Seafood Jumbo</td>\n",
       "      <td>[[Seafood, seafood]]</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The Purple Bowl</td>\n",
       "      <td>[[Breakfast &amp; Brunch, breakfast_brunch], [Acai...</td>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Mediterranean Deli</td>\n",
       "      <td>[[Greek, greek], [Mediterranean, mediterranean...</td>\n",
       "      <td>607</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Imbibe</td>\n",
       "      <td>[[American (New), newamerican], [Wine Bars, wi...</td>\n",
       "      <td>27</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sutton's Drug Store</td>\n",
       "      <td>[[Burgers, burgers], [Sandwiches, sandwiches],...</td>\n",
       "      <td>103</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.5.0. Fresh</td>\n",
       "      <td>[[American (New), newamerican]]</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Cholanad</td>\n",
       "      <td>[[Indian, indpak], [Vegan, vegan]]</td>\n",
       "      <td>261</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Cosmic Cantina</td>\n",
       "      <td>[[Tex-Mex, tex-mex], [Tacos, tacos], [Beer Bar...</td>\n",
       "      <td>103</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>R &amp; R Grill</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Burg...</td>\n",
       "      <td>93</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Crepe Traditions</td>\n",
       "      <td>[[Creperies, creperies], [Coffee &amp; Tea, coffee...</td>\n",
       "      <td>43</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Cuban Revolution Express</td>\n",
       "      <td>[[Cuban, cuban]]</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TRU Deli &amp; Wine</td>\n",
       "      <td>[[Delis, delis], [Sandwiches, sandwiches], [Co...</td>\n",
       "      <td>106</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bandido's Mexican Cafe</td>\n",
       "      <td>[[Mexican, mexican]]</td>\n",
       "      <td>79</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sandwhich</td>\n",
       "      <td>[[Sandwiches, sandwiches], [American (New), ne...</td>\n",
       "      <td>238</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Roots</td>\n",
       "      <td>[[Breakfast &amp; Brunch, breakfast_brunch], [New ...</td>\n",
       "      <td>51</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ms. Mong</td>\n",
       "      <td>[[Mongolian, mongolian], [Asian Fusion, asianf...</td>\n",
       "      <td>122</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grk Yeero</td>\n",
       "      <td>[[Mediterranean, mediterranean], [Greek, greek]]</td>\n",
       "      <td>14</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Buns</td>\n",
       "      <td>[[Burgers, burgers], [Hot Dogs, hotdog]]</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lime and Basil</td>\n",
       "      <td>[[Vietnamese, vietnamese]]</td>\n",
       "      <td>197</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sawasdee Thai Restaurant</td>\n",
       "      <td>[[Japanese, japanese], [Thai, thai]]</td>\n",
       "      <td>37</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Vespa Ristorante</td>\n",
       "      <td>[[Italian, italian], [Wine Bars, wine_bars]]</td>\n",
       "      <td>106</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Toppers Pizza</td>\n",
       "      <td>[[Chicken Wings, chicken_wings], [Pizza, pizza]]</td>\n",
       "      <td>39</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Mellow Mushroom</td>\n",
       "      <td>[[Pizza, pizza], [Sandwiches, sandwiches], [Ba...</td>\n",
       "      <td>66</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Spicy 9 Sushi Bar &amp; Asian Restaurant</td>\n",
       "      <td>[[Sushi Bars, sushi], [Asian Fusion, asianfusi...</td>\n",
       "      <td>145</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Crossroads Chapel Hill</td>\n",
       "      <td>[[American (New), newamerican], [Breakfast &amp; B...</td>\n",
       "      <td>89</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>411 West Italian Cafe</td>\n",
       "      <td>[[Italian, italian], [Breakfast &amp; Brunch, brea...</td>\n",
       "      <td>213</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Kurama Sushi &amp; Noodle Express</td>\n",
       "      <td>[[Sushi Bars, sushi], [Noodles, noodles]]</td>\n",
       "      <td>87</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ye Olde Waffle Shoppe</td>\n",
       "      <td>[[Waffles, waffles]]</td>\n",
       "      <td>129</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Spanky's Restaurant</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Brea...</td>\n",
       "      <td>101</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Top of the Hill Restaurant &amp; Brewery</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Sand...</td>\n",
       "      <td>481</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sup Dogs</td>\n",
       "      <td>[[American (New), newamerican], [Hot Dogs, hot...</td>\n",
       "      <td>83</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Benny Cappella's</td>\n",
       "      <td>[[Pizza, pizza], [Bars, bars]]</td>\n",
       "      <td>16</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Shanghai Dumplings</td>\n",
       "      <td>[[Shanghainese, shanghainese]]</td>\n",
       "      <td>92</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Artisan Pizza Kitchen</td>\n",
       "      <td>[[Pizza, pizza], [Burgers, burgers], [Sandwich...</td>\n",
       "      <td>70</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Joe's Joint</td>\n",
       "      <td>[[Burgers, burgers], [American (New), newameri...</td>\n",
       "      <td>22</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>La Residence</td>\n",
       "      <td>[[American (New), newamerican], [Venues &amp; Even...</td>\n",
       "      <td>30</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Might As Well Bar &amp; Grill</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Pizz...</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B SKI'S - The Wrap  Redefined</td>\n",
       "      <td>[[Fast Food, hotdogs], [Sandwiches, sandwiches...</td>\n",
       "      <td>83</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Time-Out Restaurant</td>\n",
       "      <td>[[Southern, southern]]</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Old Chicago Pizza &amp; Taproom</td>\n",
       "      <td>[[Pizza, pizza], [American (Traditional), trad...</td>\n",
       "      <td>113</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  \\\n",
       "11                George's Seafood Jumbo   \n",
       "33                       The Purple Bowl   \n",
       "37                    Mediterranean Deli   \n",
       "4                                 Imbibe   \n",
       "7                    Sutton's Drug Store   \n",
       "29                          1.5.0. Fresh   \n",
       "34                              Cholanad   \n",
       "15                        Cosmic Cantina   \n",
       "14                           R & R Grill   \n",
       "23                      Crepe Traditions   \n",
       "36              Cuban Revolution Express   \n",
       "10                       TRU Deli & Wine   \n",
       "8                 Bandido's Mexican Cafe   \n",
       "38                             Sandwhich   \n",
       "5                                  Roots   \n",
       "3                               Ms. Mong   \n",
       "2                              Grk Yeero   \n",
       "21                                  Buns   \n",
       "28                        Lime and Basil   \n",
       "20              Sawasdee Thai Restaurant   \n",
       "31                      Vespa Ristorante   \n",
       "32                         Toppers Pizza   \n",
       "35                       Mellow Mushroom   \n",
       "25  Spicy 9 Sushi Bar & Asian Restaurant   \n",
       "26                Crossroads Chapel Hill   \n",
       "39                 411 West Italian Cafe   \n",
       "22         Kurama Sushi & Noodle Express   \n",
       "1                  Ye Olde Waffle Shoppe   \n",
       "19                   Spanky's Restaurant   \n",
       "18  Top of the Hill Restaurant & Brewery   \n",
       "17                              Sup Dogs   \n",
       "16                      Benny Cappella's   \n",
       "13                    Shanghai Dumplings   \n",
       "9                  Artisan Pizza Kitchen   \n",
       "6                            Joe's Joint   \n",
       "24                          La Residence   \n",
       "30             Might As Well Bar & Grill   \n",
       "12         B SKI'S - The Wrap  Redefined   \n",
       "0                    Time-Out Restaurant   \n",
       "27           Old Chicago Pizza & Taproom   \n",
       "\n",
       "                                           categories  review_count  rating  \n",
       "11                               [[Seafood, seafood]]             1     5.0  \n",
       "33  [[Breakfast & Brunch, breakfast_brunch], [Acai...             8     5.0  \n",
       "37  [[Greek, greek], [Mediterranean, mediterranean...           607     4.5  \n",
       "4   [[American (New), newamerican], [Wine Bars, wi...            27     4.5  \n",
       "7   [[Burgers, burgers], [Sandwiches, sandwiches],...           103     4.5  \n",
       "29                    [[American (New), newamerican]]             2     4.5  \n",
       "34                 [[Indian, indpak], [Vegan, vegan]]           261     4.0  \n",
       "15  [[Tex-Mex, tex-mex], [Tacos, tacos], [Beer Bar...           103     4.0  \n",
       "14  [[American (Traditional), tradamerican], [Burg...            93     4.0  \n",
       "23  [[Creperies, creperies], [Coffee & Tea, coffee...            43     4.0  \n",
       "36                                   [[Cuban, cuban]]            22     4.0  \n",
       "10  [[Delis, delis], [Sandwiches, sandwiches], [Co...           106     4.0  \n",
       "8                                [[Mexican, mexican]]            79     4.0  \n",
       "38  [[Sandwiches, sandwiches], [American (New), ne...           238     4.0  \n",
       "5   [[Breakfast & Brunch, breakfast_brunch], [New ...            51     4.0  \n",
       "3   [[Mongolian, mongolian], [Asian Fusion, asianf...           122     4.0  \n",
       "2    [[Mediterranean, mediterranean], [Greek, greek]]            14     4.0  \n",
       "21           [[Burgers, burgers], [Hot Dogs, hotdog]]           293     4.0  \n",
       "28                         [[Vietnamese, vietnamese]]           197     3.5  \n",
       "20               [[Japanese, japanese], [Thai, thai]]            37     3.5  \n",
       "31       [[Italian, italian], [Wine Bars, wine_bars]]           106     3.5  \n",
       "32   [[Chicken Wings, chicken_wings], [Pizza, pizza]]            39     3.5  \n",
       "35  [[Pizza, pizza], [Sandwiches, sandwiches], [Ba...            66     3.5  \n",
       "25  [[Sushi Bars, sushi], [Asian Fusion, asianfusi...           145     3.5  \n",
       "26  [[American (New), newamerican], [Breakfast & B...            89     3.5  \n",
       "39  [[Italian, italian], [Breakfast & Brunch, brea...           213     3.5  \n",
       "22          [[Sushi Bars, sushi], [Noodles, noodles]]            87     3.5  \n",
       "1                                [[Waffles, waffles]]           129     3.5  \n",
       "19  [[American (Traditional), tradamerican], [Brea...           101     3.5  \n",
       "18  [[American (Traditional), tradamerican], [Sand...           481     3.5  \n",
       "17  [[American (New), newamerican], [Hot Dogs, hot...            83     3.5  \n",
       "16                     [[Pizza, pizza], [Bars, bars]]            16     3.5  \n",
       "13                     [[Shanghainese, shanghainese]]            92     3.5  \n",
       "9   [[Pizza, pizza], [Burgers, burgers], [Sandwich...            70     3.5  \n",
       "6   [[Burgers, burgers], [American (New), newameri...            22     3.5  \n",
       "24  [[American (New), newamerican], [Venues & Even...            30     3.0  \n",
       "30  [[American (Traditional), tradamerican], [Pizz...             5     3.0  \n",
       "12  [[Fast Food, hotdogs], [Sandwiches, sandwiches...            83     3.0  \n",
       "0                              [[Southern, southern]]            51     3.0  \n",
       "27  [[Pizza, pizza], [American (Traditional), trad...           113     2.5  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_df[['name','categories','review_count','rating']].sort_values(by='rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>review_count</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Pila</td>\n",
       "      <td>[[Cafes, cafes], [Scandinavian, scandinavian],...</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Happolati</td>\n",
       "      <td>[[Asian Fusion, asianfusion]]</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stangeriet</td>\n",
       "      <td>[[Sandwiches, sandwiches], [Meat Shops, meats]]</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Arakataka</td>\n",
       "      <td>[[Modern European, modern_european]]</td>\n",
       "      <td>23</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Nkken</td>\n",
       "      <td>[[Scandinavian, scandinavian]]</td>\n",
       "      <td>13</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Restaurant Fjord</td>\n",
       "      <td>[[Seafood, seafood]]</td>\n",
       "      <td>16</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Crperie de Mari</td>\n",
       "      <td>[[Creperies, creperies]]</td>\n",
       "      <td>30</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Meatballs</td>\n",
       "      <td>[[Fast Food, hotdogs]]</td>\n",
       "      <td>28</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Kamai</td>\n",
       "      <td>[[Sushi Bars, sushi], [Asian Fusion, asianfusi...</td>\n",
       "      <td>21</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Way Down South</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Barb...</td>\n",
       "      <td>14</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Restaurant Kontrast</td>\n",
       "      <td>[[Gastropubs, gastropubs], [Scandinavian, scan...</td>\n",
       "      <td>10</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Syverkiosken</td>\n",
       "      <td>[[Hot Dogs, hotdog], [Street Vendors, streetve...</td>\n",
       "      <td>13</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Kasbah</td>\n",
       "      <td>[[Middle Eastern, mideastern], [Gluten-Free, g...</td>\n",
       "      <td>64</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Den Glade Gris</td>\n",
       "      <td>[[Traditional Norwegian, norwegian], [Gastropu...</td>\n",
       "      <td>67</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tim Wendelboe</td>\n",
       "      <td>[[Cafes, cafes], [Coffee &amp; Tea, coffee]]</td>\n",
       "      <td>92</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pjoltergeist</td>\n",
       "      <td>[[Asian Fusion, asianfusion], [Bars, bars]]</td>\n",
       "      <td>13</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Poke Bowl Oslo</td>\n",
       "      <td>[[Fast Food, hotdogs], [Seafood, seafood], [Ha...</td>\n",
       "      <td>9</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Izakaya</td>\n",
       "      <td>[[Japanese, japanese], [Bars, bars]]</td>\n",
       "      <td>41</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Caf Tekehtopa</td>\n",
       "      <td>[[Scandinavian, scandinavian]]</td>\n",
       "      <td>24</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Freddy Fuego</td>\n",
       "      <td>[[Mexican, mexican]]</td>\n",
       "      <td>31</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ostebutikken</td>\n",
       "      <td>[[Restaurants, restaurants], [Delicatessen, de...</td>\n",
       "      <td>10</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dgnvill Bar &amp; Burger</td>\n",
       "      <td>[[Burgers, burgers], [Bars, bars]]</td>\n",
       "      <td>130</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucky Bird</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Soul...</td>\n",
       "      <td>38</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Caf Sara</td>\n",
       "      <td>[[Modern European, modern_european], [Mexican,...</td>\n",
       "      <td>56</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bon Lo</td>\n",
       "      <td>[[Spanish, spanish], [Gastropubs, gastropubs]]</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mathallen Oslo</td>\n",
       "      <td>[[Food Stands, foodstands], [Specialty Food, g...</td>\n",
       "      <td>90</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sapporo Ramenbar</td>\n",
       "      <td>[[Ramen, ramen]]</td>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>St. Lars</td>\n",
       "      <td>[[Barbeque, bbq], [Burgers, burgers]]</td>\n",
       "      <td>20</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vulkanfisk</td>\n",
       "      <td>[[Seafood, seafood], [Fish &amp; Chips, fishnchips...</td>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>La Sangria</td>\n",
       "      <td>[[Spanish, spanish], [Tapas/Small Plates, tapa...</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>East Kitchen</td>\n",
       "      <td>[[Asian Fusion, asianfusion], [Chinese, chines...</td>\n",
       "      <td>33</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Revolver</td>\n",
       "      <td>[[Restaurants, restaurants], [Music Venues, mu...</td>\n",
       "      <td>20</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Monsun Noodlebar</td>\n",
       "      <td>[[Asian Fusion, asianfusion], [Vegan, vegan], ...</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Fuglen</td>\n",
       "      <td>[[Cafes, cafes], [Coffee &amp; Tea, coffee], [Cock...</td>\n",
       "      <td>62</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sushi &amp; Dinner</td>\n",
       "      <td>[[Sushi Bars, sushi], [Japanese, japanese], [A...</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Eldhuset</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Barb...</td>\n",
       "      <td>24</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Villa Paradiso Grnerlkka</td>\n",
       "      <td>[[Italian, italian], [Pizza, pizza]]</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Colonel Mustard</td>\n",
       "      <td>[[Burgers, burgers], [Pubs, pubs]]</td>\n",
       "      <td>48</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smalhans</td>\n",
       "      <td>[[Scandinavian, scandinavian]]</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tullins Caf</td>\n",
       "      <td>[[Wok, wok], [Pizza, pizza], [Pubs, pubs]]</td>\n",
       "      <td>37</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  \\\n",
       "20                        Pila   \n",
       "15                   Happolati   \n",
       "3                   Stangeriet   \n",
       "37                   Arakataka   \n",
       "36                      Nkken   \n",
       "34            Restaurant Fjord   \n",
       "33            Crperie de Mari   \n",
       "32                   Meatballs   \n",
       "31                       Kamai   \n",
       "27              Way Down South   \n",
       "1          Restaurant Kontrast   \n",
       "17                Syverkiosken   \n",
       "16                  The Kasbah   \n",
       "39              Den Glade Gris   \n",
       "12               Tim Wendelboe   \n",
       "10                Pjoltergeist   \n",
       "7               Poke Bowl Oslo   \n",
       "13                     Izakaya   \n",
       "14              Caf Tekehtopa   \n",
       "29                Freddy Fuego   \n",
       "38                Ostebutikken   \n",
       "2        Dgnvill Bar & Burger   \n",
       "4                   Lucky Bird   \n",
       "35                   Caf Sara   \n",
       "5                      Bon Lo   \n",
       "6               Mathallen Oslo   \n",
       "8             Sapporo Ramenbar   \n",
       "30                    St. Lars   \n",
       "9                   Vulkanfisk   \n",
       "28                  La Sangria   \n",
       "26                East Kitchen   \n",
       "25                    Revolver   \n",
       "23            Monsun Noodlebar   \n",
       "22                      Fuglen   \n",
       "21              Sushi & Dinner   \n",
       "19                    Eldhuset   \n",
       "18  Villa Paradiso Grnerlkka   \n",
       "11             Colonel Mustard   \n",
       "0                     Smalhans   \n",
       "24                Tullins Caf   \n",
       "\n",
       "                                           categories  review_count  rating  \n",
       "20  [[Cafes, cafes], [Scandinavian, scandinavian],...             6     5.0  \n",
       "15                      [[Asian Fusion, asianfusion]]             6     5.0  \n",
       "3     [[Sandwiches, sandwiches], [Meat Shops, meats]]            10     5.0  \n",
       "37               [[Modern European, modern_european]]            23     4.5  \n",
       "36                     [[Scandinavian, scandinavian]]            13     4.5  \n",
       "34                               [[Seafood, seafood]]            16     4.5  \n",
       "33                           [[Creperies, creperies]]            30     4.5  \n",
       "32                             [[Fast Food, hotdogs]]            28     4.5  \n",
       "31  [[Sushi Bars, sushi], [Asian Fusion, asianfusi...            21     4.5  \n",
       "27  [[American (Traditional), tradamerican], [Barb...            14     4.5  \n",
       "1   [[Gastropubs, gastropubs], [Scandinavian, scan...            10     4.5  \n",
       "17  [[Hot Dogs, hotdog], [Street Vendors, streetve...            13     4.5  \n",
       "16  [[Middle Eastern, mideastern], [Gluten-Free, g...            64     4.5  \n",
       "39  [[Traditional Norwegian, norwegian], [Gastropu...            67     4.5  \n",
       "12           [[Cafes, cafes], [Coffee & Tea, coffee]]            92     4.5  \n",
       "10        [[Asian Fusion, asianfusion], [Bars, bars]]            13     4.5  \n",
       "7   [[Fast Food, hotdogs], [Seafood, seafood], [Ha...             9     4.5  \n",
       "13               [[Japanese, japanese], [Bars, bars]]            41     4.5  \n",
       "14                     [[Scandinavian, scandinavian]]            24     4.0  \n",
       "29                               [[Mexican, mexican]]            31     4.0  \n",
       "38  [[Restaurants, restaurants], [Delicatessen, de...            10     4.0  \n",
       "2                  [[Burgers, burgers], [Bars, bars]]           130     4.0  \n",
       "4   [[American (Traditional), tradamerican], [Soul...            38     4.0  \n",
       "35  [[Modern European, modern_european], [Mexican,...            56     4.0  \n",
       "5      [[Spanish, spanish], [Gastropubs, gastropubs]]             8     4.0  \n",
       "6   [[Food Stands, foodstands], [Specialty Food, g...            90     4.0  \n",
       "8                                    [[Ramen, ramen]]            13     4.0  \n",
       "30              [[Barbeque, bbq], [Burgers, burgers]]            20     4.0  \n",
       "9   [[Seafood, seafood], [Fish & Chips, fishnchips...            13     4.0  \n",
       "28  [[Spanish, spanish], [Tapas/Small Plates, tapa...            12     4.0  \n",
       "26  [[Asian Fusion, asianfusion], [Chinese, chines...            33     4.0  \n",
       "25  [[Restaurants, restaurants], [Music Venues, mu...            20     4.0  \n",
       "23  [[Asian Fusion, asianfusion], [Vegan, vegan], ...            22     4.0  \n",
       "22  [[Cafes, cafes], [Coffee & Tea, coffee], [Cock...            62     4.0  \n",
       "21  [[Sushi Bars, sushi], [Japanese, japanese], [A...            17     4.0  \n",
       "19  [[American (Traditional), tradamerican], [Barb...            24     4.0  \n",
       "18               [[Italian, italian], [Pizza, pizza]]            97     4.0  \n",
       "11                 [[Burgers, burgers], [Pubs, pubs]]            48     4.0  \n",
       "0                      [[Scandinavian, scandinavian]]            39     4.0  \n",
       "24         [[Wok, wok], [Pizza, pizza], [Pubs, pubs]]            37     3.5  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oslo_df = get_yelp_results('Oslo, Norway')\n",
    "oslo_df[['name','categories','review_count','rating']].sort_values(by='rating', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Your turn. Modify the function to take different kinds of business. \n",
    "\n",
    "You can also add a category of business to search for from the [list](https://www.yelp.com/developers/documentation/v2/category_list) of acceptable values. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Your turn. Sign up to be a developer on Twitter. Figure out the next steps...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The function expects that the first thing you input will be a location. Taking advantage of both `oath2`'s ability to clean up the text so that it is functional when put in a URL (e.g., escape spaces) and Yelp's savvy ability to parse locations, the value for location can be fairly wide (e.g., \"Chapel Hill\" or \"90210\"). You can also add a category of business to search for from the [list](https://www.yelp.com/developers/documentation/v2/category_list) of acceptable values. If you don't provide a value, `category_filter = 'restaurants'` provides a default value of 'restaurants'. This function returns the JSON formatted results. Note that this doesn't have any mechanism for handling errors, which will need to happen elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
